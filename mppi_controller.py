import numpy as np

class MppiController:
    """Klasa implementujÄ…ca sterownik bazujÄ…cy na metodzie MPPI"""

    def __init__(self, model, ref_path, N=30, K=300, lambda_=0.8, dt=0.05, noise_sigma=(0.2, 0.5)):
        """ Inicjalizacja parametrÃ³w sterowania

        :param model: obiekt klasy VehicleModel
        :param N: dÅ‚ugoÅ›Ä‡ horyzontu (w krokach)
        :param K: liczba trajektorii - rollouts
        :param lambda_: parametr eksploracji
        :param dt: krok symulacji
        :param noise_sigma: wariancje szumu sterowania (dla delta_dot i T_dot)
        """

        self.model = model
        self.N = N
        self.K = K
        self.lambda_ = lambda_
        self.dt = dt
        self.noise_sigma = np.array(noise_sigma)

        self.udim = 2                                   # [delta_dot, T_dot]
        self.nominal_u = np.zeros((N, self.udim))       # bieÅ¼Ä…ca trajektoria sterowania
        self.ref_path = ref_path

                                                        # for visualization
        self.last_rollouts = []
        self.last_nominal = None

    def simulate_trajectory(self, x0, U):
        """Symuluje trajektoriÄ™ dla danego ciÄ…gu sterowaÅ„"""

        x = np.copy(x0)
        trajectory = [x.copy()]             # dodaje stan poczÄ…tkowy x0 do trajektorii

        for u in U:
            #u = np.array([u[0], max(0.0, u[1])])  # â›” nie pozwÃ³l MPPI cofaÄ‡

            u[0] = np.clip(u[0], -0.5236, 0.5236)
            u[1] = np.clip(u[1], -3 , 3 )
            x = self.model.next_state(x, u, self.dt)

            trajectory.append(x.copy())
        return np.array(trajectory)
    
    def compute_cost(self, trajectory):
        total_cost = 0.0

        for state in trajectory:
            x, y, yaw, v = state

            if v < 0.01:
                return 1e6

            distances = np.linalg.norm(self.ref_path - np.array([x, y]), axis=1)
            nearest_idx = np.argmin(distances)

            # ðŸ“Œ zamiast punktu najbliÅ¼szego, celuj w punkt do przodu
            lookahead_idx = min(nearest_idx + 3, len(self.ref_path) - 1)
            target_point = self.ref_path[lookahead_idx]

            # dystans
            min_dist = np.linalg.norm(target_point - np.array([x, y]))

            # kierunek toru
            if lookahead_idx < len(self.ref_path) - 1:
                next_point = self.ref_path[lookahead_idx + 1]
            else:
                next_point = target_point

            path_direction = np.arctan2(next_point[1] - target_point[1],
                                        next_point[0] - target_point[0])
            yaw_diff = np.arctan2(np.sin(yaw - path_direction), np.cos(yaw - path_direction))

            total_cost += (
                3.0 * min_dist ** 2 +
                5.5 * yaw_diff ** 2 +
                1.0 * v **2
            )

            # if v < 0.2:
            #     total_cost += (0.2 - v) ** 2 * 50  # kara roÅ›nie, im wolniej

        return total_cost

   


    def control(self, x0):
        """Zwraca najlepsze sterowanie na podstawie aktualnego stanu"""
        
        # Perturbacje sterowania (Î´u) - KaÅ¼dy rollout dostaje unikalny zestaw szumÃ³w.
        noises = np.random.normal(0, self.noise_sigma, size=(self.K, self.N, self.udim))
        
        # Inicjalizacja wektora funkcji kosztu trajektorii
        costs = np.zeros(self.K)

        #Monte Carlo rollouts
        for k in range(self.K):
            u_k = self.nominal_u + noises[k]
            trajectory = self.simulate_trajectory(x0, u_k)
            costs[k] = self.compute_cost(trajectory)

        
        # WaÅ¼one poprawki do sterowania
        beta = np.min(costs)                                        # przesuniÄ™cie dla stabilnoÅ›ci numerycznej
        weights = np.exp(-1.0 / self.lambda_ * (costs - beta))      # (costs - beta) daje najniÅ¼szemu kosztowi wagÄ™ â‰ˆ 1
        weights /= np.sum(weights + 1e-6)                           # normalizacja, by wagi dawaÅ‚y sumÄ™ â‰ˆ 1
        
        delta_u =  np.sum(weights[:, None, None] * noises, axis=0)  # delta_u = sum_k w_k * Î´u_k  -> reward-weighted perturbations
        self.nominal_u += delta_u

        # Zastosowanie u0 i przesuniÄ™cie trajektorii
        u_cmd = self.nominal_u[0]

        self.nominal_u[:-1] = self.nominal_u[1:]    # aktualizacja: usuniÄ™cie wykorzystanego sterowania (u0) i wstawienie 0.0
        self.nominal_u[-1] = 0.0

        # Zapisz trajektorie do wizualizacji
        # self.last_nominal = self.simulate_trajectory(x0, self.nominal_u)

        # # wybierz najlepsze rollouty
        # sorted_idxs = np.argsort(costs)
        # best_idxs = sorted_idxs[:10]

        # self.last_rollouts = []
        # for idx in best_idxs:
        #     u_k = self.nominal_u + noises[idx]
        #     traj = self.simulate_trajectory(x0, u_k)
        #     self.last_rollouts.append(traj)

        #print(f"u_cmd: steer={u_cmd[0]:+.3f}, accel={u_cmd[1]:+.3f}")

        u_cmd[1] = max(0.0, u_cmd[1]) 
        return u_cmd