# Opis problemu: Sterowanie systemami nieliniowymi z zakÅ‚Ã³ceniami

## RÃ³wnanie dynamiki systemu

System jest opisywany rÃ³wnaniem rÃ³Å¼niczkowym stochastycznym:

$$
dx = \big(f(x, t) + G(x, t)u(x, t)\big)dt + B(x, t)d\omega
$$

* $x$ â€“ stan systemu
* $u(x, t)$ â€“ sterowanie (wejÅ›cie)
* $f(x, t)$ â€“ nieliniowa funkcja dynamiki
* $G(x, t)$ â€“ macierz wejÅ›ciowa (wpÅ‚yw sterowania)
* $B(x, t)d\omega$ â€“ zakÅ‚Ã³cenie procesowe (szum, np. modelowany jako proces Wienerowski)

System jest nieliniowy i zawiera zakÅ‚Ã³cenia (ang. *process noise*).

---

## Schemat blokowy

* **System z zakÅ‚Ã³ceniami** generuje stan $x_k$.

* **Sensor** obserwuje stan, ale zwraca tylko szacowanÄ… wartoÅ›Ä‡ z zakÅ‚Ã³ceniem pomiarowym:
  $z_k + \zeta_k$
  gdzie $\zeta_k$ to szum pomiarowy.

* Pomiar trafia do **kontrolera**, ktÃ³ry:

  * Oblicza sterowanie $u_k$
  * UÅ¼ywa funkcji kosztu (ang. *cost function*), by zoptymalizowaÄ‡ decyzje.

* Sterowanie $u_k$ jest nastÄ™pnie podawane do systemu.

---

## ReguÅ‚a aktualizacji sterowania (Update Law)

Zaktualizowane sterowanie w danym stanie i czasie obliczane jest zgodnie z poniÅ¼szym wzorem:

$$
\mathbf{u}(x_t, t)^* = \mathbf{u}(x_t, t) + \frac{\mathbb{E}_q\left[\exp\left(-\frac{1}{\lambda} \tilde{S}(\tau)\right) \delta \mathbf{u}\right]}{\mathbb{E}_q\left[\exp\left(-\frac{1}{\lambda} \tilde{S}(\tau)\right)\right]}
$$

gdzie:

-  $\mathbf{u}(x_t, t)$  â€” bieÅ¼Ä…ce (nominalne) sterowanie,
- $ \delta \mathbf{u}$  â€” perturbacja sterowania,
- $\tilde{S}(\tau)$  â€” skumulowany koszt trajektorii \( $\tau$ \),
- $\lambda $ â€” parametr temperatury (kontroluje stopieÅ„ eksploracji),
- $\mathbb{E}_q[\cdot]$  â€” wartoÅ›Ä‡ oczekiwana wzglÄ™dem rozkÅ‚adu trajektorii.

### Struktura wzoru:

$$
\text{optymalne sterowanie} =
\underbrace{\text{nominalne sterowanie}}_{\text{plan bazowy}} +
\underbrace{
\frac{\text{Å›rednia waÅ¼ona perturbacji}}{\text{suma wag}}
}_{\text{uÅ›redniona poprawka}}
$$


To formalna definicja aktualizacji sterowania w algorytmie Model Predictive Path Integral (MPPI) w ujÄ™ciu probabilistycznym. Sterowanie stanowi sumÄ™ nominalnego wejÅ›cia i Å›redniej waÅ¼onej perturbacji, gdzie wagi sÄ… funkcjÄ… (malejÄ…cÄ…) kosztu trajektorii.

Obliczasz wartoÅ›Ä‡ oczekiwanÄ… z perturbacji, waÅ¼onÄ… przez ekspotencjalnie przeskalowany koszt


### SkÄ…d siÄ™ bierze podana we wzorze wartoÅ›Ä‡ oczekiwana ? 
Ten wzÃ³r pochodzi z tzw. path integral control theory (czyli "sterowania przez caÅ‚kÄ™ po Å›cieÅ¼kach").
To podejÅ›cie przeksztaÅ‚ca problem optymalnego sterowania w probabilistyczny problem uÅ›redniania po trajektoriach, waÅ¼onych przez ich jakoÅ›Ä‡.

```
Zamiast klasycznego podejÅ›cia opartego na bezpoÅ›redniej minimalizacji funkcjonaÅ‚u kosztu, metodÄ™ formuÅ‚uje siÄ™ jako problem probabilistycznego uÅ›redniania: generuje siÄ™ zestaw trajektorii, wyznacza ich skumulowany koszt, a nastÄ™pnie aktualizuje sterowanie poprzez Å›redniÄ… waÅ¼onÄ… perturbacji, gdzie wagi sÄ… wyznaczone na podstawie przeksztaÅ‚conych kosztÃ³w (np. w formie rozkÅ‚adu Boltzmannowskiego).
```

---

## Cel

ZaprojektowaÄ‡ **efektywny kontroler** dla systemu nieliniowego w obecnoÅ›ci **szumu procesowego i zakÅ‚Ã³ceÅ„ pomiarowych**, wykorzystujÄ…c **zaawansowanÄ… teoriÄ™ sterowania stochastycznego**, np. path integral control.

---

# MPPI â€“ Kluczowa idea algorytmu (wizualizacja)

## ğŸ” Opis ogÃ³lny

**Model Predictive Path Integral (MPPI)**, naleÅ¼y do klasy sterowania predykcyjnego (MPC), ale oparty jest na podejÅ›ciu stochastycznym (*sampling-based*).

### â™» GÅ‚Ã³wne kroki algorytmu:

1. **Symulacja przyszÅ‚oÅ›ci** â€“ tworzonych jest wiele tzw. *rolloutÃ³w*, czyli trajektorii.
2. **Losowe zakÅ‚Ã³cenia wejÅ›Ä‡** â€“ kaÅ¼da trajektoria ma nieco inne sterowanie: `u + delta_u_k`.
3. **Ocena trajektorii** â€“ kaÅ¼da trajektoria otrzymuje koszt `S_k` (im niÅ¼szy, tym lepiej).
4. **Wyznaczenie najlepszego wejÅ›cia** â€“ nowe sterowanie obliczane jest jako waÅ¼ona suma perturbacji `delta_u_k`, gdzie wagi `w_k` zaleÅ¼Ä… od kosztu `S_k`.

---

## SzczegÃ³Å‚y matematyczne

### Uaktualnianie wejÅ›cia

FormuÅ‚a aktualizacji sterowania:


$$
\mathbf{u} = \mathbf{u} + \frac{\sum_{k=1}^K w_k \delta \mathbf{u}_k}{\sum_{k=1}^K w_k}
$$


gdzie:

* $\delta \mathbf{u_k} $ â€“ zakÅ‚Ã³cenie sterowania w rollout'cie `k`,
* $ \mathbf{w_k} = \exp(-S_k / \lambda) $ â€“ waga zaleÅ¼na od kosztu,
* $\mathbf{\lambda}$ â€“ parametr eksploracji (temperatura),
* $\mathbf{S_k}$ â€“ koszt trajektorii `k`.

### Dlaczego dodajemy tylko perturbacje $\delta u$ do $u$, zamiast nadpisywaÄ‡ $u$ nowÄ… wartoÅ›ciÄ…?
Bo MPPI to algorytm iteracyjny, a nie bezpoÅ›redni optymalizator.

MPPI nie szuka "od zera" najlepszej sekwencji sterowania w kaÅ¼dej iteracji.
Zamiast tego:
- startuje od aktualnego planu (nominal_u)
- eksploruje jego otoczenie poprzez dodanie losowych szumÃ³w (perturbacji $\delta u_k$)
- uczy siÄ™, w ktÃ³rÄ… stronÄ™ warto poprawiÄ‡ â€” na podstawie kosztÃ³w
- aktualizuje lekko â€” tylko o $\delta u$

Czyli dziaÅ‚a jak lokalny, przybliÅ¼ony, stochastyczny gradient.
---

## Interpretacja wizualna

<img src="../images/Core_idea_mppi.png" width="100%"/>

* Na wykresie przedstawiono 3 trajektorie (`delta_u_1`, `delta_u_2`, `delta_u_3`) w rÃ³Å¼nych kolorach.
* KaÅ¼da ma przypisany **koszt**: 100, 2000, 5000.
* Im **niÅ¼szy koszt**, tym **wiÄ™ksza waga perturbacji** i **wiÄ™kszy wpÅ‚yw na wynikowe sterowanie**.

---

## ğŸ“Œ Kluczowe pojÄ™cia z diagramu

| Symbol / hasÅ‚o    | Znaczenie                               |
| ----------------- | --------------------------------------- |
| `u`               | Nominalne (bieÅ¼Ä…ce) sterowanie          |
| `delta_u_k`       | ZakÅ‚Ã³cenie sterowania w rollout'cie `k` |
| `S_k`             | Koszt trajektorii rollout'u `k`         |
| `w_k`             | Waga perturbacji                        |
| `lambda`          | Parametr eksploracji (temperatura)      |
| Normalize weights | Normalizacja wag do sumy â‰ˆ 1            |

---

## âš™ï¸ Schemat algorytmu MPPI (pseudo-kod)

```
Initialize control sequence u_0..u_{N-1}
while task not completed do:
    Generate random perturbations Î´u
    for Monte Carlo rollouts k = 1..K do:
        start in current state x_k = x(t_0)
        for MPC horizon steps n = 0..N-1 do:
            Input u_{k,n} = u_n + Î´u_{k,n}
            Next state x_{k,n+1} = model(x_{k,n}, u_{k,n})
            Rollout cost S_k += stage cost q_{k,n}
        end
    end
    for n = 0..N-1 do:
        u_n += reward-weighted perturbations
    end
    Apply first input u_0
    Get system feedback
    Check if task completed
end

```

## ğŸ“Š Kluczowe kroki algorytmu MPPI z wyrÃ³Å¼nieniem

1. **Inicjalizacja sterowania:**
   `u_0..N-1` â€“ inicjalna sekwencja sterowania (np. zerowa lub nominalna).

2. **GÅ‚Ã³wna pÄ™tla sterowania:**
   `while task not completed:` â€“ wykonuj pÄ™tlÄ™, aÅ¼ zadanie zostanie zakoÅ„czone.

3. **Losowanie perturbacji sterowania:**
   `Generate random perturbations Î´u` â€“ losowe odchylenia wejÅ›Ä‡ dla prÃ³b Monte Carlo.

4. **Symulacja trajektorii (Monte Carlo rollouts):**
   `for k = 1..K:` â€“ wykonaj **K rolloutÃ³w**, czyli symulowanych przebiegÃ³w przyszÅ‚oÅ›ci.
   `x_k = x(t0)` â€“ kaÅ¼dy rollout startuje ze stanu poczÄ…tkowego.

5. **Symulacja w horyzoncie predykcji (MPC horizon):**
   `for n = 0..N-1:`
   Â Â Â Â `u_k,n = u_n + Î´u_k,n` â€“ **dodanie perturbacji do nominalnego sterowania**.
   Â Â Â Â `x_k,n+1 = model(x_k,n, u_k,n)` â€“ **symulacja nowego stanu** na podstawie modelu.
   Â Â Â Â `S_k += stage cost q_k,n` â€“ **sumowanie kosztu etapu** dla trajektorii.

6. **Aktualizacja sterowania:**
   `u_n += reward-weighted perturbations`
   â†’ ğŸ”¥ **To najwaÅ¼niejszy krok!** Aktualizacja wejÅ›cia na podstawie trajektorii, ktÃ³re miaÅ‚y najmniejsze koszty.

7. **Zastosowanie pierwszego sterowania:**
   `Apply first input u_0` â€“ wyÅ›lij pierwsze sterowanie do systemu.

8. **SprzÄ™Å¼enie zwrotne:**
   `Get system feedback` â€“ pobierz nowe dane z sensora/systemu.

9. **Sprawdzenie warunku zakoÅ„czenia:**
   `Check if task completed`

## ğŸ”— Å¹rÃ³dÅ‚o

Wzory oraz koncepcje pochodzÄ… z wykÅ‚adu gdzie zaprezentowany zostaÅ‚ sposÃ³b implementacji MPPI: [MPPI Lecture on YouTube](https://www.youtube.com/watch?v=19QLyMuQ_BE&t=1107s)